{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1968a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import helper\n",
    "\n",
    "from cnn_model import cnn_model\n",
    "from loss_functions import symmetric_cross_entropy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor_estimator and flc (still wait for check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3588eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3422 - loss: 1.1023 - val_accuracy: 0.3483 - val_loss: 1.0981\n",
      "Epoch 2/5\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.3410 - loss: 1.0982 - val_accuracy: 0.3487 - val_loss: 1.0970\n",
      "Epoch 3/5\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3491 - loss: 1.0975 - val_accuracy: 0.3373 - val_loss: 1.0976\n",
      "Epoch 4/5\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3502 - loss: 1.0967 - val_accuracy: 0.3630 - val_loss: 1.0971\n",
      "Epoch 5/5\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3599 - loss: 1.0945 - val_accuracy: 0.3657 - val_loss: 1.0951\n",
      "Estimated Transition Matrix (T_hat):\n",
      "[[0.44752827 0.37157804 0.28800556]\n",
      " [0.34708044 0.36276808 0.29623663]\n",
      " [0.20539126 0.26565382 0.4157578 ]]\n",
      "------------------------------\n",
      "Matrix shape: (3, 3)\n",
      "Column sums: [1. 1. 1.]\n",
      "Mean diagonal value: 0.4087\n",
      "Mean off-diagonal value: 0.1971\n"
     ]
    }
   ],
   "source": [
    "# anchor_estimator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from helper import load_dataset, split_data\n",
    "from cnn_model import cnn_model\n",
    "from anchor_estimator import temperature_scale_probs, estimate_T_anchor_from_probs\n",
    "from loss_functions import symmetric_cross_entropy\n",
    "from flc_loss import forward_correction_loss\n",
    "\n",
    "\n",
    "def ensure_column_stochastic(T: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    T = np.clip(T, 0, None)\n",
    "    colsum = T.sum(axis=0, keepdims=True) + eps\n",
    "    return T / colsum\n",
    "\n",
    "\n",
    "Xtr, Str, Xts, Yts, T_true = load_dataset(\"./datasets/CIFAR.npz\", \"CIFAR.npz\")\n",
    "\n",
    "# just avoid the loss from mismatch of onehot/float \n",
    "Str = Str.astype(\"int64\")\n",
    "Yts = Yts.astype(\"int64\")\n",
    "\n",
    "Xtr = Xtr.astype(\"float32\") / 255.0\n",
    "Xts = Xts.astype(\"float32\") / 255.0\n",
    "\n",
    "X_tr, y_tr, X_val, y_val = split_data(Xtr, Str, train_ratio=0.8, random_seed=7)\n",
    "\n",
    "num_classes = int(np.max(Str)) + 1\n",
    "input_shape = Xtr.shape[1:]\n",
    "\n",
    "# 2. Warm-up process, after estimation, the CIFAR.npz is almost same noise 0.6\n",
    "alpha, beta, A = 0.05, 4.0, -4.0   \n",
    "# sce_loss = symmetric_cross_entropy(alpha=alpha, beta=beta, A=A, num_classes=num_classes)\n",
    "\n",
    "m = cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "m.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "          loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "m.fit(X_tr, y_tr,\n",
    "      validation_data=(X_val, y_val),\n",
    "      epochs=5, batch_size=128, verbose=1)\n",
    "\n",
    "# 3. use the val datasets to get the matrix\n",
    "p_val = m.predict(X_val, batch_size=128, verbose=0)              \n",
    "p_val_cal, bestT = temperature_scale_probs(p_val, y_val)          \n",
    "T_hat = estimate_T_anchor_from_probs(p_val_cal, top_quantile=0.99)  \n",
    "T_hat = ensure_column_stochastic(T_hat).astype(np.float32)\n",
    "\n",
    "# Transition Matrix (3×3) \n",
    "print(\"Estimated Transition Matrix (T_hat):\")\n",
    "\n",
    "print(T_hat)                        \n",
    "print(\"------------------------------\")\n",
    "print(\"Matrix shape:\", T_hat.shape) \n",
    "\n",
    "# Check the sum of each col\n",
    "print(\"Column sums:\", np.sum(T_hat, axis=0)) \n",
    "\n",
    "# diagonal value and off-diagonal value\n",
    "diag_mean = np.mean(np.diag(T_hat))\n",
    "off_mean = np.mean(T_hat - np.diag(np.diag(T_hat)))\n",
    "print(f\"Mean diagonal value: {diag_mean:.4f}\")\n",
    "print(f\"Mean off-diagonal value: {off_mean:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e683acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.3543 - loss: 1.1077 - val_accuracy: 0.3613 - val_loss: 1.0961\n",
      "Epoch 2/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3600 - loss: 1.0954 - val_accuracy: 0.3607 - val_loss: 1.0965\n",
      "Epoch 3/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3649 - loss: 1.0954 - val_accuracy: 0.3567 - val_loss: 1.0973\n",
      "Epoch 4/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.3745 - loss: 1.0944 - val_accuracy: 0.3703 - val_loss: 1.0951\n",
      "Epoch 5/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3665 - loss: 1.0952 - val_accuracy: 0.3657 - val_loss: 1.0968\n",
      "Epoch 6/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.3687 - loss: 1.0946 - val_accuracy: 0.3660 - val_loss: 1.0961\n",
      "Epoch 7/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3712 - loss: 1.0926 - val_accuracy: 0.3597 - val_loss: 1.0949\n",
      "Epoch 8/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.3704 - loss: 1.0925 - val_accuracy: 0.3543 - val_loss: 1.0963\n",
      "Epoch 9/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3686 - loss: 1.0919 - val_accuracy: 0.3537 - val_loss: 1.0953\n",
      "Epoch 10/10\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.3787 - loss: 1.0895 - val_accuracy: 0.3527 - val_loss: 1.0950\n",
      "[FLC] Test Accuracy: 0.6307\n"
     ]
    }
   ],
   "source": [
    "# Forward Correction fine-tuning\n",
    "flc_loss = forward_correction_loss(T_hat, num_classes=num_classes)\n",
    "m_flc = cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "m_flc.set_weights(m.get_weights())\n",
    "m_flc.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=flc_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "history_flc = m_flc.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = m_flc.evaluate(Xts, Yts, verbose=0)\n",
    "print(f\"[FLC] Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "debf9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, dataset, method=\"fc\", transition_matrix=None, epochs=50, input_shape=(28, 28, 1), num_classes=3):\n",
    "    \n",
    "    model = cnn_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "    if method == \"sce\":\n",
    "        if dataset == \"FashionMNIST0.3\":\n",
    "            alpha = 0.01\n",
    "            beta = 1\n",
    "        elif dataset == \"FashionMNIST0.6\":\n",
    "            alpha = 0.01\n",
    "            beta = 1\n",
    "        elif dataset == \"CIFAR\":\n",
    "            alpha = 0.1\n",
    "            beta = 1\n",
    "        A=-4.0\n",
    "        loss_function = symmetric_cross_entropy(alpha=alpha, beta=beta, A=A, num_classes=num_classes)\n",
    "    # elif method == \"forward\":\n",
    "    #     #forward function\n",
    "    # elif method == \"coteaching\":\n",
    "    #     #coteaching function\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = loss_function,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_classes == y_test) * 100\n",
    "    return accuracy\n",
    "\n",
    "def run_single_experiment(Xtr, Str, Xts, Yts, T, dataset, method, num_runs=10, epochs=50):\n",
    "    Xtr = Xtr.astype('float32') / 255.0\n",
    "    Xts = Xts.astype('float32') / 255.0\n",
    "    input_shape = Xtr.shape[1:] \n",
    "    \n",
    "    if method == 'fc':\n",
    "        if T is not None:\n",
    "            transition_matrix = T\n",
    "        else:\n",
    "            #call estimate T function here\n",
    "            pass\n",
    "    else:\n",
    "        transition_matrix=None\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        seed = RANDOM_SEED + run\n",
    "\n",
    "        X_train, y_train, X_val, y_val = helper.split_data(\n",
    "            Xtr, Str, train_ratio=0.8, random_seed=seed\n",
    "        )\n",
    "\n",
    "        model = train_model(X_train, y_train, X_val, y_val, dataset=dataset, method=method, transition_matrix=transition_matrix, epochs=epochs, input_shape=input_shape, num_classes=3)\n",
    "\n",
    "        accuracy = evaluate_model(model, Xts, Yts)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Run {run+1}/{num_runs}: Test Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    return accuracies\n",
    "    \n",
    "def run_all_experiments(datasets, methods, num_runs=10, epochs=50):\n",
    "    results = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for method in methods:\n",
    "            print(f\"Running {method.upper()} on {dataset}...\")\n",
    "\n",
    "            data_path = f'datasets/{dataset}.npz'\n",
    "\n",
    "            Xtr, Str, Xts, Yts, T = helper.load_dataset(data_path, dataset) \n",
    "            accuracies = run_single_experiment(\n",
    "                Xtr, Str, Xts, Yts, T, dataset, method, num_runs, epochs\n",
    "            )\n",
    "            mean_acc = np.mean(accuracies)\n",
    "            std_acc = np.std(accuracies)\n",
    "\n",
    "            results.append({\n",
    "                'Dataset': dataset,\n",
    "                'Method': method.upper(),\n",
    "                'Mean': mean_acc,\n",
    "                'Std': std_acc,\n",
    "                'Result': f\"{mean_acc:.2f} ± {std_acc:.2f}\"\n",
    "            })\n",
    "\n",
    "            print(f\"Result: {mean_acc:.2f} ± {std_acc:.2f}%\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c682d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SCE on FashionMNIST0.3...\n",
      "Run 1/10: Test Accuracy = 98.53%\n",
      "Run 2/10: Test Accuracy = 98.67%\n",
      "Run 3/10: Test Accuracy = 98.37%\n",
      "Run 4/10: Test Accuracy = 98.77%\n",
      "Run 5/10: Test Accuracy = 98.80%\n",
      "Run 6/10: Test Accuracy = 98.53%\n",
      "Run 7/10: Test Accuracy = 98.37%\n",
      "Run 8/10: Test Accuracy = 98.63%\n",
      "Run 9/10: Test Accuracy = 98.73%\n",
      "Run 10/10: Test Accuracy = 98.40%\n",
      "Result: 98.58 ± 0.16%\n",
      "Running SCE on FashionMNIST0.6...\n",
      "Run 1/10: Test Accuracy = 96.13%\n",
      "Run 2/10: Test Accuracy = 96.27%\n",
      "Run 3/10: Test Accuracy = 96.03%\n",
      "Run 4/10: Test Accuracy = 96.13%\n",
      "Run 5/10: Test Accuracy = 96.43%\n",
      "Run 6/10: Test Accuracy = 94.97%\n",
      "Run 7/10: Test Accuracy = 96.83%\n",
      "Run 8/10: Test Accuracy = 95.50%\n",
      "Run 9/10: Test Accuracy = 95.17%\n",
      "Run 10/10: Test Accuracy = 94.87%\n",
      "Result: 95.83 ± 0.63%\n",
      "Running SCE on CIFAR...\n",
      "Run 1/10: Test Accuracy = 67.87%\n",
      "Run 2/10: Test Accuracy = 67.60%\n",
      "Run 3/10: Test Accuracy = 64.07%\n",
      "Run 4/10: Test Accuracy = 62.67%\n",
      "Run 5/10: Test Accuracy = 64.67%\n",
      "Run 6/10: Test Accuracy = 68.50%\n",
      "Run 7/10: Test Accuracy = 66.60%\n",
      "Run 8/10: Test Accuracy = 67.90%\n",
      "Run 9/10: Test Accuracy = 57.73%\n",
      "Run 10/10: Test Accuracy = 68.13%\n",
      "Result: 65.57 ± 3.22%\n"
     ]
    }
   ],
   "source": [
    "datasets = ['FashionMNIST0.3', 'FashionMNIST0.6', 'CIFAR']\n",
    "methods = ['sce'] #add more methods here\n",
    "\n",
    "result = run_all_experiments(datasets, methods, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6574152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method                    SCE\n",
      "Dataset                      \n",
      "CIFAR            65.57 ± 3.22\n",
      "FashionMNIST0.3  98.58 ± 0.16\n",
      "FashionMNIST0.6  95.83 ± 0.63\n"
     ]
    }
   ],
   "source": [
    "pivot_df = result.pivot(index='Dataset', columns='Method', values='Result')\n",
    "    \n",
    "print(pivot_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
